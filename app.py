import os
import re
import time
import random
import argparse
from collections import Counter
import numpy as np
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

# --- Configuration ---
SEED = 42
# Default model path - can be overridden by command line argument
DEFAULT_MODEL_PATH = '/kaggle/input/gemma-3/transformers/gemma-3-27b-it/1'
DEFAULT_ANSWER = 210 # Default fallback answer

# --- Environment Setup ---
# os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3" # Control GPU visibility if needed
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# --- Global Variables ---
model = None
tokenizer = None

# --- Core Functions ---

def seed_everything(seed=SEED):
    """
    Sets random seeds for reproducibility across multiple libraries.

    Args:
        seed (int): The seed value to use.
    """
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed) # Use manual_seed_all for multi-GPU
    # Deterministic ops can slow down, enable if strict reproducibility is paramount
    # torch.use_deterministic_algorithms(True)
    # torch.backends.cudnn.benchmark = False # Disable benchmarking for determinism
    # torch.backends.cudnn.deterministic = True # Use deterministic algorithms
    torch.backends.cudnn.benchmark = True # Keep benchmark True for performance
    torch.backends.cudnn.deterministic = False
    print(f"Random seed set to {seed}")

def initialize_model(model_path):
    """
    Loads the Gemma language model and tokenizer from the specified path.

    Initializes the global `model` and `tokenizer` variables.

    Args:
        model_path (str): The path to the directory containing the model weights
                          and tokenizer files.

    Returns:
        bool: True if initialization was successful, False otherwise.
    """
    global model, tokenizer
    if model is not None and tokenizer is not None:
        print("Model already initialized.")
        return True

    print(f"Loading Gemma model from: {model_path}")
    if not os.path.isdir(model_path):
        print(f"Error: Model path not found or is not a directory: {model_path}")
        return False

    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)

        # Gemma usually doesn't have a separate pad token; use eos_token
        if tokenizer.pad_token is None:
            print("Setting pad_token to eos_token")
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id

        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16, # Use bfloat16 for compatibility and performance
            device_map="auto",         # Automatically distribute across available GPUs
            low_cpu_mem_usage=True     # Optimization for loading large models
        )

        # Configure generation settings
        model.generation_config = GenerationConfig.from_pretrained(model_path)
        model.generation_config.pad_token_id = tokenizer.pad_token_id # Ensure pad token is set

        torch.cuda.empty_cache() # Clear cache after loading
        print("Gemma model loaded successfully.")
        return True

    except Exception as e:
        print(f"Error loading model: {e}")
        model = None
        tokenizer = None
        return False

def extract_boxed_answer(text):
    """
    Extracts the final numerical answer (modulo 1000) from the generated text.

    Prioritizes answers enclosed in \\boxed{}, with fallbacks for other patterns.

    Args:
        text (str): The text generated by the language model.

    Returns:
        int: The extracted answer (0-999), or DEFAULT_ANSWER if extraction fails.
    """
    # Primary pattern: \boxed{...}
    pattern = r'\\boxed{([^{}]+)}'
    matches = re.findall(pattern, text)
    if matches:
        for match in reversed(matches): # Check last match first
            match_strip = match.strip().replace(',', '') # Remove commas
            # Simple integer check
            if match_strip.lstrip('-').isdigit(): # Allow negative sign
                try:
                    return int(match_strip) % 1000
                except ValueError:
                    continue # Should not happen with isdigit, but safety first
            # Try extracting numbers if not a direct integer (e.g., contains calculation)
            number_pattern = r'(-?\d+)' # Allow negative numbers before modulo
            numbers = re.findall(number_pattern, match_strip)
            if numbers:
                try:
                    # Take the last number found within the box
                    ans = int(numbers[-1])
                    return ans % 1000
                except ValueError:
                    continue # Try next match if conversion fails

    # Fallback patterns if \boxed{} fails or is missing
    # Search from the end of the text backwards for robustness
    text_lines = text.split('\n')
    final_patterns = [
        r'[Tt]he final answer is.*?(-?\d+)',
        r'[Tt]he answer is.*?(-?\d+)',
        r'[Aa]nswer:.*?(-?\d+)',
        r'\$?(\-?\d+)\$? is the final answer',
        r'\$?(\-?\d+)\$? final', # Match numbers near 'final'
        r'= (-?\d+) \(mod 1000\)', # Specific modulo patterns
        r'\\equiv (-?\d+) \(mod 1000\)',
        r'is (-?\d+)\.\s*$', # Number followed by period at end of line/string
        r'\$?(-?\d+)\$?\.?\s*$', # Last number in a line, optionally with $ or .
    ]

    for line in reversed(text_lines[-5:]): # Check last 5 lines
        line_strip = line.strip().replace(',', '') # Remove commas
        if not line_strip: continue

        for pattern in final_patterns:
            matches = re.findall(pattern, line_strip)
            if matches:
                try:
                    # Take the last match found on the line
                    ans = int(matches[-1])
                    return ans % 1000
                except ValueError:
                    continue

        # Last resort: Find the last number in the last few lines if context words exist
        if any(kw in line_strip.lower() for kw in ['answer', 'result', 'final', 'solution', 'value is']):
             numbers = re.findall(r'(-?\d+)', line_strip)
             if numbers:
                 try:
                    ans = int(numbers[-1])
                    return ans % 1000
                 except ValueError:
                     continue

    # Fallback if absolutely nothing is found
    print("Warning: Could not extract a numerical answer reliably. Using default.")
    return DEFAULT_ANSWER

def identify_problem_type(question):
    """
    Performs a simple keyword-based classification of the mathematical problem type.

    Args:
        question (str): The text of the mathematical problem.

    Returns:
        str: A string representing the identified type (e.g., 'geometry',
             'number_theory', 'algebra', 'combinatorics', 'general').
    """
    q_lower = question.lower()
    if any(k in q_lower for k in ['triangle', 'circle', 'polygon', 'angle', 'geometry', 'point', 'line', 'area', 'volume', 'perpendicular', 'parallel', 'radius', 'diameter']):
        return 'geometry'
    if any(k in q_lower for k in ['prime', 'divisor', 'gcd', 'lcm', 'modulo', 'congruence', 'integer', 'number theory', 'divisible', 'remainder']):
        return 'number_theory'
    if any(k in q_lower for k in ['algebra', 'equation', 'polynomial', 'sequence', 'series', 'function', 'solve for x', 'variable', 'sum', 'product', 'recurrence', 'inequality']):
        return 'algebra'
    if any(k in q_lower for k in ['combinatorics', 'probability', 'count', 'choose', 'permutation', 'arrangement', 'graph', 'dice', 'card', 'ways']):
        return 'combinatorics'
    return 'general'

def create_gemma_prompt_step(current_context, step_instruction):
    """
    Formats a prompt for a single reasoning step using Gemma's instruction format.

    Args:
        current_context (str): The history of the conversation/reasoning so far,
                               including previous model responses.
        step_instruction (str): The specific instruction for the current step
                                (e.g., "Step 1: Identify givens...").

    Returns:
        str: The fully formatted prompt ready for the Gemma model.
    """
    # The structure is based on alternating user/model turns.
    # The new instruction is appended as the latest user request.
    prompt = f"{current_context}<start_of_turn>user\n{step_instruction}<end_of_turn>\n<start_of_turn>model\n"
    return prompt

def generate_solution_iterative(question, max_step_tokens=1024, max_final_tokens=1536, temperature=0.6, top_p=0.95):
    """
    Generates a mathematical solution step-by-step using the loaded Gemma model.

    Follows a predefined sequence of reasoning steps, prompting the model
    iteratively.

    Args:
        question (str): The mathematical problem text.
        max_step_tokens (int): Max new tokens to generate for intermediate steps.
        max_final_tokens (int): Max new tokens for the final solution step.
        temperature (float): Sampling temperature for generation.
        top_p (float): Nucleus sampling probability.

    Returns:
        tuple[str, int]: A tuple containing:
            - full_conversation (str): The entire conversation history, including
              all prompts and model responses.
            - final_answer (int): The extracted final answer (0-999 or default).
        Returns (None, DEFAULT_ANSWER) if the model is not initialized.
    """
    if model is None or tokenizer is None:
        print("Error: Model not initialized. Cannot generate solution.")
        return None, DEFAULT_ANSWER

    problem_type = identify_problem_type(question)
    print(f"Identified problem type: {problem_type}")

    # Define reasoning steps (adapted from original logic)
    steps = [
        "Step 1: Understand the problem. Identify the knowns, unknowns, and the goal. What type of math is involved?",
        "Step 2: Brainstorm potential approaches. List relevant theorems, formulas, or techniques. Consider edge cases.",
        "Step 3: Select the most promising approach and outline the solution steps.",
        "Step 4: Execute the solution step-by-step, showing all calculations clearly. Double-check intermediate results.",
        "Step 5: Perform the final calculation. Ensure the answer is an integer. Calculate the result modulo 1000. If the result is negative, add 1000.",
        "Step 6: Final Verification. Does the answer make sense? Briefly review the steps. State the final answer clearly using \\boxed{integer}." # Added explicit boxing instruction
    ]

    # Initial context setup for Gemma (first user turn)
    initial_prompt = f"Solve the following mathematical problem step-by-step. Show your reasoning clearly.\n\nProblem:\n{question}"
    full_conversation = f"<start_of_turn>user\n{initial_prompt}<end_of_turn>\n"

    # Generation config setup
    gen_config = model.generation_config
    gen_config.temperature = temperature
    gen_config.top_p = top_p
    gen_config.top_k = 50 # Keep or adjust
    gen_config.repetition_penalty = 1.1 # Keep or adjust
    gen_config.do_sample = True if temperature > 0 else False
    gen_config.num_return_sequences = 1

    # Process each step iteratively
    for i, step_text in enumerate(steps):
        print(f"\n--- Generating {step_text[:20]}... ---")
        torch.cuda.empty_cache() # Clear cache before each step

        # Create prompt for the current step using Gemma format
        step_prompt = create_gemma_prompt_step(full_conversation, step_text)

        # Tokenize the *entire* conversation history up to this point
        inputs = tokenizer(step_prompt, return_tensors="pt", padding=False)
        input_ids = inputs.input_ids.to(model.device)

        # Adjust max tokens for the step
        gen_config.max_new_tokens = max_final_tokens if i == len(steps) - 1 else max_step_tokens

        step_response_text = ""
        try:
            with torch.no_grad():
                outputs = model.generate(
                    input_ids,
                    generation_config=gen_config,
                )

            # Decode only the *newly generated part*
            output_ids = outputs[0][input_ids.shape[1]:]
            step_response_text = tokenizer.decode(output_ids, skip_special_tokens=True).strip()

            # Append the model's response to the conversation history
            full_conversation += f"{step_response_text}<end_of_turn>\n" # Append response and end turn marker

            # Debug: Print snippet of the response
            print(f"Step {i+1} Response Snippet: ...{step_response_text[-200:]}")

            # Clean up memory
            del outputs
            torch.cuda.empty_cache()

        except Exception as e:
            print(f"Error during generation step {i+1}: {e}")
            # Decide how to handle errors: stop, skip step, use default?
            # For now, just print error and continue if possible, or break
            full_conversation += f"Error during step {i+1}.<end_of_turn>\n" # Log error in conversation
            # break # Option to stop on error

    # Extract the final answer from the *entire* conversation
    final_answer = extract_boxed_answer(full_conversation)

    return full_conversation, final_answer

def solve_problem(question, model_path, **gen_kwargs):
    """
    High-level function to solve a single mathematical problem.

    Initializes the model if necessary, generates the solution iteratively,
    and returns the results.

    Args:
        question (str): The mathematical problem text.
        model_path (str): Path to the model directory.
        **gen_kwargs: Additional keyword arguments passed to
                      generate_solution_iterative (e.g., temperature).

    Returns:
        tuple[str, int]: A tuple containing:
            - full_conversation (str): The complete generation trace.
            - final_answer (int): The extracted final answer.
        Returns (None, DEFAULT_ANSWER) on model initialization failure.
    """
    # Initialize model if not already done
    if not initialize_model(model_path):
        return "Model initialization failed.", DEFAULT_ANSWER

    print(f"\nSolving problem: {question[:150]}...")
    start_solve_time = time.time()

    full_conversation, final_answer = generate_solution_iterative(question, **gen_kwargs)

    end_solve_time = time.time()
    print(f"Problem solving took {end_solve_time - start_solve_time:.2f} seconds.")

    return full_conversation, final_answer

# --- Main Execution ---

def main():
    """
    Main function to parse arguments and run the solver.
    """
    parser = argparse.ArgumentParser(description="Solve mathematical problems using a Gemma model.")
    parser.add_argument("-p", "--problem", type=str, required=True,
                        help="The mathematical problem text to solve.")
    parser.add_argument("-m", "--model_path", type=str, default=DEFAULT_MODEL_PATH,
                        help=f"Path to the Gemma model directory (default: {DEFAULT_MODEL_PATH})")
    parser.add_argument("-t", "--temperature", type=float, default=0.6,
                        help="Generation temperature (default: 0.6)")
    parser.add_argument("--top_p", type=float, default=0.95,
                        help="Nucleus sampling top_p (default: 0.95)")
    parser.add_argument("--seed", type=int, default=SEED,
                        help=f"Random seed for reproducibility (default: {SEED})")
    parser.add_argument("--max_step_tokens", type=int, default=1024,
                        help="Max new tokens per intermediate step (default: 1024)")
    parser.add_argument("--max_final_tokens", type=int, default=1536,
                        help="Max new tokens for the final step (default: 1536)")
    parser.add_argument("--show_reasoning", action="store_true",
                        help="Print the full reasoning steps generated by the model.")

    args = parser.parse_args()

    # Set seed
    seed_everything(args.seed)

    # Solve the problem
    full_reasoning, final_answer = solve_problem(
        args.problem,
        args.model_path,
        temperature=args.temperature,
        top_p=args.top_p,
        max_step_tokens=args.max_step_tokens,
        max_final_tokens=args.max_final_tokens
    )

    print("\n" + "="*30 + " Solution " + "="*30)
    if args.show_reasoning and full_reasoning:
        print("\n--- Full Reasoning Trace ---")
        # Basic cleanup of Gemma turn markers for readability
        readable_reasoning = full_reasoning.replace("<start_of_turn>user\n","\n--- User Prompt ---\n")
        readable_reasoning = readable_reasoning.replace("<start_of_turn>model\n","\n--- Model Response ---\n")
        readable_reasoning = readable_reasoning.replace("<end_of_turn>\n","\n")
        print(readable_reasoning)
        print("--- End of Trace ---\n")

    if final_answer is not None:
        print(f"Problem: {args.problem}")
        print(f"Extracted Final Answer (mod 1000): {final_answer}")
    else:
        print("Could not determine the final answer.")
    print("="*60)


if __name__ == "__main__":
    main()